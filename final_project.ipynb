{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0-q7OrqoJAkl"
   },
   "outputs": [],
   "source": [
    "# sentiment_analysis_model.py\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "nHMEtX9MESSy",
    "outputId": "b9283216-f34d-4737-cad8-c4c9ca4a1da5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.8488\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.88      0.85      4961\n",
      "    positive       0.87      0.82      0.85      5039\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4361  600]\n",
      " [ 912 4127]]\n",
      "\n",
      "Predictions for Example Reviews:\n",
      "Review: 'This movie is amazing! I loved every moment of it.'\n",
      "Sentiment: positive\n",
      "\n",
      "Review: 'The plot was confusing, and the acting was not great.'\n",
      "Sentiment: negative\n",
      "\n",
      "Review: 'A masterpiece! The cinematography and acting were superb.'\n",
      "Sentiment: positive\n",
      "\n",
      "Review: 'I couldn't follow the storyline, and the characters were poorly developed.'\n",
      "Sentiment: negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the IMDb dataset\n",
    "df = pd.read_csv('imdb-dataset.csv')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline with CountVectorizer and Naive Bayes classifier\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model Accuracy: {accuracy}')\n",
    "\n",
    "# Print additional analysis\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Save the model to a .pkl file\n",
    "joblib.dump(model, 'sentiment_analysis_model.pkl', protocol=4)\n",
    "\n",
    "# Test the model with various inputs\n",
    "example_reviews = [\n",
    "    \"This movie is amazing! I loved every moment of it.\",\n",
    "    \"The plot was confusing, and the acting was not great.\",\n",
    "    \"A masterpiece! The cinematography and acting were superb.\",\n",
    "    \"I couldn't follow the storyline, and the characters were poorly developed.\"\n",
    "]\n",
    "\n",
    "print(\"\\nPredictions for Example Reviews:\")\n",
    "for review in example_reviews:\n",
    "    prediction = model.predict([review])[0]\n",
    "    print(f\"Review: '{review}'\\nSentiment: {prediction}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skZ6neC3HeVb"
   },
   "source": [
    "Time Consumption:\n",
    "\n",
    "a. Timing Predictions: Use the time module to measure the time it takes to make predictions on the test data. This is crucial, especially if your model needs to be deployed in a real-time setting.\n",
    "\n",
    "b. Batch Processing: If you plan to deploy this model in a production environment, consider testing its performance with a batch of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QHL38WakG5Va"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for predictions: 4.743443012237549 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "y_pred = model.predict(X_test)\n",
    "end_time = time.time()\n",
    "\n",
    "time_taken = end_time - start_time\n",
    "print(f\"Time taken for predictions: {time_taken} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COy8O5RPHacS"
   },
   "source": [
    "Cross-Validation:\n",
    "\n",
    "a. Cross-Validation Scores: Instead of a single train-test split, you might perform k-fold cross-validation to get a more robust estimate of your model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SqO-ViewG6S6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.84575  0.850875 0.85675  0.840125 0.846   ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5)  # 5-fold cross-validation\n",
    "print(\"Cross-Validation Scores:\", scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SnWz7U9HWyc"
   },
   "source": [
    "Hyperparameter Tuning:\n",
    "\n",
    "a. Grid Search: If applicable, you might perform a grid search to find the optimal hyperparameters for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-j4f85AqG8KW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'multinomialnb__alpha': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'multinomialnb__alpha': [0.1, 1.0, 10.0]}\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhGMK3YPHsAE"
   },
   "source": [
    "Accuracy:\n",
    "\n",
    "Formula:\n",
    "Number of Correct Predictions\n",
    "____________________________________\n",
    "Total Number of Predictions\n",
    "​\n",
    "\n",
    "Interpretation: The proportion of correctly classified instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9mj-Cm6HrD6"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODb8i4iRH7kO"
   },
   "source": [
    "Precision:\n",
    "\n",
    "Formula:\n",
    "True Positives\n",
    "_________________________________\n",
    "True Positives + False Positives\n",
    "​\n",
    "\n",
    "Interpretation: The ability of the model to avoid labeling negative instances as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBua7hqHIFIA"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred, pos_label='positive')  # Specify positive label\n",
    "print(f'Precision: {precision}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMbDN9WeIHVm"
   },
   "source": [
    "Recall (Sensitivity):\n",
    "\n",
    "Formula:\n",
    "True Positives\n",
    "_____________________________________\n",
    "True Positives + False Negatives\n",
    "​\n",
    "\n",
    "Interpretation: The ability of the model to identify all relevant instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-tGuIp3IT_r"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "recall = recall_score(y_test, y_pred, pos_label='positive')  # Specify positive label\n",
    "print(f'Recall: {recall}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hfCAH-OIXHu"
   },
   "source": [
    "F1-Score:\n",
    "\n",
    "Formula:\n",
    "2 ×  Precision × Recall\n",
    "    ____________________\n",
    "      Precision + Recall\n",
    "\n",
    "​\n",
    "\n",
    "Interpretation: The harmonic mean of precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4UzsJlrkItwj"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, pos_label='positive')  # Specify positive label\n",
    "print(f'F1-Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGp8OpQiIwwr"
   },
   "source": [
    "Confusion Matrix:\n",
    "\n",
    "Interpretation: A table that shows the number of true positives, true negatives, false positives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5pZ0y0tI05b"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKPs0kkRI231"
   },
   "source": [
    "Receiver Operating Characteristic (ROC) Curve and Area Under the Curve (AUC):\n",
    "\n",
    "Interpretation: Useful for binary classification problems. The ROC curve shows the trade-off between true positive rate and false positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Yt2n2XRI6A-"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:, 1], pos_label='positive')\n",
    "auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
